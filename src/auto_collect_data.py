"""
VSL Auto Collector Holistic from JSON - MediaPipe Task API
PhiÃªn báº£n "Super Augmentation Holistic": Táº¡o >35 biáº¿n thá»ƒ (Tay + Máº·t + DÃ¡ng) tá»« 1 video.
"""

import cv2
import numpy as np
import os
import json
import math
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

class VSLAutoCollector:
    def __init__(self, json_path, output_dir='../data/raw'):
        self.output_dir = output_dir
        self.json_path = json_path
        self.sequence_length = 30 
        
        # Táº¡o thÆ° má»¥c lÆ°u data
        os.makedirs(output_dir, exist_ok=True)

        print("Initializing MediaPipe Holistic (Auto Collector)...")
        self._setup_models()
        self._init_detectors()

    def _setup_models(self):
        models = {
            'hand_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',
            'face_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
            'pose_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task'
        }
        import urllib.request
        for name, url in models.items():
            if not os.path.exists(name):
                print(f"Downloading {name}...")
                try:
                    urllib.request.urlretrieve(url, name)
                except Exception as e:
                    print(f"âŒ Failed to download {name}: {e}")

    def _init_detectors(self):
        # Hand
        base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')
        options = vision.HandLandmarkerOptions(
            base_options=base_options, num_hands=2, min_hand_detection_confidence=0.3)
        self.hand_detector = vision.HandLandmarker.create_from_options(options)

        # Face
        base_options = python.BaseOptions(model_asset_path='face_landmarker.task')
        options = vision.FaceLandmarkerOptions(
            base_options=base_options, num_faces=1)
        self.face_detector = vision.FaceLandmarker.create_from_options(options)

        # Pose
        base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')
        options = vision.PoseLandmarkerOptions(base_options=base_options)
        self.pose_detector = vision.PoseLandmarker.create_from_options(options)

    def process_json(self, target_list=None, limit=5):
        """Äá»c file JSON vÃ  xá»­ lÃ½ video."""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Xá»­ lÃ½ Ä‘á»‹nh dáº¡ng JSON List hay Dict
            video_list = data if isinstance(data, list) else data.get('words', [])
            
            data_to_process = []

            # LOGIC Lá»ŒC Tá»ª
            if target_list and len(target_list) > 0:
                print(f"ğŸ¯ Äang tÃ¬m kiáº¿m cÃ¡c tá»«: {target_list}")
                targets_lower = [t.lower().strip() for t in target_list]
                
                for item in video_list:
                    gloss = item.get('gross', '').strip()
                    if gloss.lower() in targets_lower:
                        data_to_process.append(item)
                
                if len(data_to_process) == 0:
                    print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y tá»« nÃ o trong danh sÃ¡ch yÃªu cáº§u!")
                    return
            else:
                data_to_process = video_list[:limit]
            
            print(f"âœ… TÃ¬m tháº¥y {len(data_to_process)} video phÃ¹ há»£p. Báº¯t Ä‘áº§u xá»­ lÃ½...")
            
            for index, item in enumerate(data_to_process):
                gloss = item.get('gross')
                url = item.get('url')
                
                if gloss and url:
                    safe_name = gloss.replace(" ", "_").lower()
                    print(f"\n[{index+1}/{len(data_to_process)}] Äang há»c tá»«: '{gloss}'...")
                    self.process_single_video(safe_name, url)
                
        except Exception as e:
            print(f"Lá»—i khi xá»­ lÃ½ JSON: {e}")
            import traceback
            traceback.print_exc()

    def process_single_video(self, sign_name, video_url):
        cap = cv2.VideoCapture(video_url)
        
        if not cap.isOpened():
            print(f"âŒ KhÃ´ng thá»ƒ má»Ÿ video: {video_url}")
            return

        raw_sequence = [] 
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
            
            # Detect Holistic
            hand_res = self.hand_detector.detect(mp_image)
            face_res = self.face_detector.detect(mp_image)
            pose_res = self.pose_detector.detect(mp_image)
            
            # Extract combined keypoints
            kps = self.extract_keypoints(hand_res, face_res, pose_res)
            # Normalize (Optional: currently returning raw relative coords)
            norm_kps = self.normalize_keypoints(kps) 
            raw_sequence.append(kps)
            
        cap.release()

        if len(raw_sequence) < 10:
            print(f"âš ï¸ Video quÃ¡ ngáº¯n ({len(raw_sequence)} frames). Bá» qua.")
            return

        # Táº¡o augmentation (PhiÃªn báº£n Holistic)
        self.generate_augmentations(sign_name, raw_sequence)

    def extract_keypoints(self, hand_result, face_result, pose_result):
        """Extract combined keypoints: Pose(99) + Face(1434) + Hands(126) = 1659"""
        keypoints = []
        
        # 1. Pose (33 * 3)
        if pose_result.pose_landmarks:
            for landmark in pose_result.pose_landmarks[0]:
                keypoints.extend([landmark.x, landmark.y, landmark.z])
        else:
            keypoints.extend([0] * 99)
            
        # 2. Face (478 * 3)
        if face_result.face_landmarks:
            for landmark in face_result.face_landmarks[0]:
                keypoints.extend([landmark.x, landmark.y, landmark.z])
        else:
            keypoints.extend([0] * 1434)
            
        # 3. Hands (21 * 2 * 3)
        hand_kps = []
        if hand_result.hand_landmarks:
            for hand_landmarks in hand_result.hand_landmarks:
                for landmark in hand_landmarks:
                    hand_kps.extend([landmark.x, landmark.y, landmark.z])
        while len(hand_kps) < 126:
            hand_kps.extend([0] * 63)
        keypoints.extend(hand_kps[:126])
        
        return keypoints

    def resample_sequence(self, sequence, target_len):
        if len(sequence) == target_len:
            return np.array(sequence)
        
        resampled = []
        sequence = np.array(sequence)
        length = len(sequence)
        indices = np.linspace(0, length - 1, target_len)
        
        for i in indices:
            low = int(math.floor(i))
            high = int(math.ceil(i))
            weight = i - low
            
            if high >= length:
                resampled.append(sequence[length-1])
            else:
                frame = sequence[low] * (1 - weight) + sequence[high] * weight
                resampled.append(frame)
                
        return np.array(resampled)

    # ==========================================================
    # ğŸš€ SUPER AUGMENTATION ENGINE (Holistic Version)
    # ==========================================================
    def apply_rotation(self, data_reshaped, angle):
        """HÃ m phá»¥ trá»£ Ä‘á»ƒ xoay dá»¯ liá»‡u 3D"""
        rad = np.radians(angle)
        cos_a, sin_a = np.cos(rad), np.sin(rad)
        rot_matrix = np.array([[cos_a, -sin_a], [sin_a, cos_a]])
        
        rot_data = data_reshaped.copy()
        # Chá»‰ xoay toáº¡ Ä‘á»™ X, Y (2 cá»™t Ä‘áº§u tiÃªn cá»§a dimension cuá»‘i)
        xy_coords = rot_data[:, :, :2] 
        rot_xy = np.dot(xy_coords, rot_matrix)
        rot_data[:, :, :2] = rot_xy
        
        # Flatten láº¡i vá» vector Ä‘áº·c trÆ°ng (1659)
        return rot_data.reshape(self.sequence_length, -1)

    def generate_augmentations(self, sign_name, raw_sequence):
        save_path = os.path.join(self.output_dir, sign_name)
        os.makedirs(save_path, exist_ok=True)
        
        base_data = self.resample_sequence(raw_sequence, self.sequence_length)
        
        # Shape chuáº©n Holistic: (30, 553, 3) vÃ¬ 1659 / 3 = 553 Ä‘iá»ƒm landmark
        num_landmarks = base_data.shape[1] // 3
        base_data_reshaped = base_data.reshape(self.sequence_length, num_landmarks, 3) 

        augmentations = []

        # === PHáº¦N 1: Dá»® LIá»†U Gá»C & CÆ  Báº¢N (2 file) ===
        augmentations.append(("org", base_data))
        
        noise = np.random.normal(0, 0.002, base_data.shape)
        augmentations.append(("noise", base_data + noise))

        # === PHáº¦N 2: CÃC BIáº¾N THá»‚ HÃŒNH Há»ŒC (Gá»C) ===
        angles = [-10, -5, 5, 10]        # 4 gÃ³c xoay
        scales = [0.9, 0.95, 1.05, 1.1] # 4 má»©c co giÃ£n
        shifts = [                      # 4 má»©c dá»‹ch chuyá»ƒn
            (0.02, 0), (-0.02, 0),
            (0, 0.02), (0, -0.02)
        ]

        # 2.1 Xoay (4 file)
        for angle in angles:
            aug_data = self.apply_rotation(base_data_reshaped, angle)
            augmentations.append((f"rot{angle}", aug_data))

        # 2.2 Scale (4 file)
        for scale in scales:
            aug_data = base_data * scale
            augmentations.append((f"scale{scale}", aug_data))

        # 2.3 Shift (4 file)
        for idx, (sx, sy) in enumerate(shifts):
            shift_data = base_data.copy()
            # Cá»™ng shift cho X vÃ  Y (giáº£ Ä‘á»‹nh normalized)
            # Reshape táº¡m Ä‘á»ƒ cá»™ng Ä‘Ãºng cá»™t
            temp = shift_data.reshape(self.sequence_length, -1, 3)
            temp[:, :, 0] += sx
            temp[:, :, 1] += sy
            augmentations.append((f"shift{idx}", temp.reshape(self.sequence_length, -1)))

        # === PHáº¦N 3: Láº¬T GÆ¯Æ NG (Flip) ===
        mirror_reshaped = base_data_reshaped.copy()
        mirror_reshaped[:, :, 0] = -mirror_reshaped[:, :, 0] # Äáº£o trá»¥c X
        mirror_flat = mirror_reshaped.reshape(self.sequence_length, -1)
        
        augmentations.append(("flip_org", mirror_flat))

        # 3.1 Flip + Xoay (4 file)
        for angle in angles:
            aug_data = self.apply_rotation(mirror_reshaped, -angle) 
            augmentations.append((f"flip_rot{angle}", aug_data))

        # LÆ¯U FILE
        count = 0
        for suffix, data in augmentations:
            filename = f"{sign_name}_{suffix}.npy"
            file_path = os.path.join(save_path, filename)
            np.save(file_path, data.astype(np.float32))
            count += 1
            
        print(f"   -> ÄÃ£ táº¡o {count} file training (Holistic Augmentation) táº¡i: {save_path}")


if __name__ == "__main__":
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(current_dir, 'data.json')
    output_dir = os.path.join(current_dir, '../data/raw')

    # ==========================================
    # ğŸ“ DANH SÃCH Tá»ª Báº N MUá»N Há»ŒC Táº I ÄÃ‚Y
    # ==========================================
    words_to_learn = [
        "vui má»«ng", 
        "buá»•i sÃ¡ng", 
        "cáº£m Æ¡n",
        "Ä‘á»‹a chá»‰",
        "xin lá»—i",
        "táº¡m biá»‡t"
    ]

    print(f"Äang Ä‘á»c data tá»«: {json_path}")
    
    if os.path.exists(json_path):
        collector = VSLAutoCollector(json_path=json_path, output_dir=output_dir)
        collector.process_json(target_list=words_to_learn)
    else:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {json_path}")
